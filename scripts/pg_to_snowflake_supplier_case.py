"""
MGTA 464 / ETL-ELT Case — End-to-end Python + Snowflake Script
Group Members: Trevor Sullivan, Abdullah AlJarallah, and Leo (Yuxuan) Tong
---------------------------------------------------------------

This script implements (and documents) the Part 2 tasks from the ETL/ELT (Snowflake and Python) assignment:
- Creates Snowflake objects (warehouse, DB, schema; Task pre-reqs)
- Stages and loads CSV purchase order line-item data into Snowflake database via PUT/COPY (Task 2)
- Shreds/loads XML supplier invoice data (Task 3)
- Creates views for PO header & invoice selection, then joins (Task 4–5)
- Extracts Postgres supplier_case to local CSV, stages, creates table & loads (Task 6a)
- (Task 6b) **Schema inference from CSV → CREATE TABLE** via a helper function
  **NOTE:** The task 6b helper function is clearly labeled as "Generated by ChatGPT" below.
- Creates a materialized view that joins geocoded supplier ZIPs and weather stations that report "maximum temperature" via CROSS JOIN (Task 7)
- Joins transaction data × supplier data × weather data (Task 8)

Throughout this project, the code prioritizes ELT in Snowflake (SQL via cs.execute) and avoids
unnecessary Pandas transformations.

"""

# Snowflake connection (Part 2 pre-reqs)
import snowflake.connector

conn = snowflake.connector.connect(
    user="trsullivan", password="Memphis043002!", account="CYWNEAB-AWB56568"
)
cs = conn.cursor()

# Environment setup in Snowflake
# Use an admin-capable role to create objects below (WAREHOUSE/DB/SCHEMA).
cs.execute("USE ROLE ACCOUNTADMIN")

# Create & select a small warehouse for this work
cs.execute("CREATE WAREHOUSE IF NOT EXISTS MY_FIRST_WAREHOUSE")
cs.execute("USE WAREHOUSE MY_FIRST_WAREHOUSE")

# Create & select a working database/schema
cs.execute("CREATE DATABASE IF NOT EXISTS TESTDB")
cs.execute("USE DATABASE TESTDB")
cs.execute("USE SCHEMA PUBLIC")


# Task 2 — Purchases data (41 CSVs): Stage → COPY → Strongly-typed table

# (2.1) Before staging the CSV files, we started by creating a destination table with specified column names and datatypes to hold transformed PO line data.
cs.execute(
    "CREATE OR REPLACE TABLE my_table (PurchaseOrderID NUMBER,SupplierID NUMBER,OrderDate DATE,DeliveryMethodID NUMBER,ContactPersonID NUMBER,ExpectedDeliveryDate DATE,SupplierReference STRING,IsOrderFinalized BOOLEAN,Comments STRING,InternalComments STRING,LastEditedBy NUMBER,LastEditedWhen TIMESTAMP_NTZ,PurchaseOrderLineID NUMBER,StockItemID NUMBER,OrderedOuters NUMBER,Description STRING,ReceivedOuters NUMBER,PackageTypeID NUMBER,ExpectedUnitPricePerOuter NUMBER(18,4),LastReceiptDate DATE,IsOrderLineFinalized BOOLEAN,Right_LastEditedBy NUMBER,Right_LastEditedWhen TIMESTAMP_NTZ)"
)

# (2.2) We then created a raw landing table with only strings to avoid formatting issues on initial load.
cs.execute("""
CREATE OR REPLACE TABLE MY_TABLE_RAW (
  PurchaseOrderID            STRING,
  SupplierID                 STRING,
  OrderDate                  STRING,
  DeliveryMethodID           STRING,
  ContactPersonID            STRING,
  ExpectedDeliveryDate       STRING,
  SupplierReference          STRING,
  IsOrderFinalized           STRING,
  Comments                   STRING,
  InternalComments           STRING,
  LastEditedBy               STRING,
  LastEditedWhen             STRING,
  PurchaseOrderLineID        STRING,
  StockItemID                STRING,
  OrderedOuters              STRING,
  Description                STRING,
  ReceivedOuters             STRING,
  PackageTypeID              STRING,
  ExpectedUnitPricePerOuter  STRING,
  LastReceiptDate            STRING,
  IsOrderLineFinalized       STRING,
  Right_LastEditedBy         STRING,
  Right_LastEditedWhen       STRING
)
""")

# (2.3) We then staged all monthly CSVs into the table-local stage (%MY_TABLE_RAW) via PUT.
cs.execute(
    "PUT file:///home/jovyan/msba/mgta464/Data/MonthlyPOData/*.csv @%MY_TABLE_RAW/inbox/ OVERWRITE=TRUE AUTO_COMPRESS=TRUE"
)

# (2.4) Loaded the CSVs into RAW (parse header; flexible quoting; ignore blanks).
cs.execute("""
COPY INTO MY_TABLE_RAW
FROM @%MY_TABLE_RAW/inbox/
FILE_FORMAT = (
  TYPE = CSV
  PARSE_HEADER = TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY = '"'
  TRIM_SPACE = TRUE
  NULL_IF = ('','NULL')
  EMPTY_FIELD_AS_NULL = TRUE
)
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE
PATTERN = '.*\\.csv(\\.gz)?'
ON_ERROR = CONTINUE
FORCE = TRUE
""")

# (2.5) Finally, we transformed the raw table into a typed table with consistent formatting (dates, numbers, booleans).
# Note the explicit TRY_… casts and fallbacks to handle inconsistent source formats.
# Note this query was debugged with the help of ChatGPT.
cs.execute("""
INSERT INTO MY_TABLE (
  PurchaseOrderID, SupplierID, OrderDate, DeliveryMethodID, ContactPersonID,
  ExpectedDeliveryDate, SupplierReference, IsOrderFinalized, Comments, InternalComments,
  LastEditedBy, LastEditedWhen, PurchaseOrderLineID, StockItemID, OrderedOuters,
  Description, ReceivedOuters, PackageTypeID, ExpectedUnitPricePerOuter, LastReceiptDate,
  IsOrderLineFinalized, Right_LastEditedBy, Right_LastEditedWhen
)
SELECT
  TO_NUMBER(PurchaseOrderID),
  TO_NUMBER(SupplierID),
  TRY_TO_DATE(OrderDate, 'MM/DD/YYYY'),                 -- CHANGED
  TO_NUMBER(DeliveryMethodID),
  TO_NUMBER(ContactPersonID),
  TRY_TO_DATE(ExpectedDeliveryDate, 'MM/DD/YYYY'),      -- CHANGED
  SupplierReference,
  TO_BOOLEAN(IsOrderFinalized),
  Comments,
  InternalComments,
  TO_NUMBER(LastEditedBy),
  COALESCE(
    TRY_TO_TIMESTAMP_NTZ(LastEditedWhen, 'MM/DD/YYYY HH24:MI'),
    TRY_TO_TIMESTAMP_NTZ(LastEditedWhen)
  ),
  TO_NUMBER(PurchaseOrderLineID),
  TO_NUMBER(StockItemID),
  TO_NUMBER(OrderedOuters),
  Description,
  TO_NUMBER(ReceivedOuters),
  TO_NUMBER(PackageTypeID),
  TO_DECIMAL(ExpectedUnitPricePerOuter, 18, 4),
  TRY_TO_DATE(LastReceiptDate, 'MM/DD/YYYY'),           -- CHANGED
  TO_BOOLEAN(IsOrderLineFinalized),
  TO_NUMBER(Right_LastEditedBy),
  COALESCE(
    TRY_TO_TIMESTAMP_NTZ(Right_LastEditedWhen, 'MM/DD/YYYY HH24:MI'),
    TRY_TO_TIMESTAMP_NTZ(Right_LastEditedWhen)
  )
FROM MY_TABLE_RAW
""")

# Quick verification
cs.execute("SELECT COUNT(*) FROM MY_TABLE")
print("final rows:", cs.fetchone()[0])
cs.execute("SELECT COUNT(*) FROM MY_TABLE_RAW")
print("raw rows:  ", cs.fetchone()[0])
cs.execute("SELECT * FROM MY_TABLE LIMIT 10")
print(cs.fetchall())
cs.execute("SELECT * FROM MY_TABLE_RAW LIMIT 40")
print(cs.fetchall())
cs.execute("SELECT * FROM my_table_raw")
print(cs.fetchmany(2))

"""After loading all 41 CSVs into one table, we ended up with 8,367 rows."""

# Task 3 — Supplier invoice XML: stage → shred → typed table

# (3.1) We created another raw table for the 2021 ZCTA TXT file (used later with weather mapping).
cs.execute("""
CREATE OR REPLACE TABLE ZCTA_2021_RAW (
  GEOID        STRING,
  ALAND        STRING,
  AWATER       STRING,
  ALAND_SQMI   STRING,
  AWATER_SQMI  STRING,
  INTPTLAT     STRING,
  INTPTLONG    STRING
)
""")

# We then staged and loaded the ZCTA tab-delimited file.
cs.execute("""
PUT file:///home/jovyan/msba/mgta464/Data/2021_Gaz_zcta_national.txt
  @%ZCTA_2021_RAW/inbox/
  OVERWRITE=TRUE
  AUTO_COMPRESS=TRUE
""")
cs.execute("""
CREATE OR REPLACE FILE FORMAT TSV_GAZ
  TYPE=CSV
  FIELD_DELIMITER='\\t'
  PARSE_HEADER=TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY='\"'
  TRIM_SPACE=TRUE
  EMPTY_FIELD_AS_NULL=TRUE
  NULL_IF=('','NULL')
""")
cs.execute("""
COPY INTO ZCTA_2021_RAW
FROM @%ZCTA_2021_RAW/inbox/
FILE_FORMAT=(FORMAT_NAME=TSV_GAZ)
MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE
PATTERN='.*\\.txt(\\.gz)?'
ON_ERROR=CONTINUE
FORCE=TRUE
""")

# Strongly-typed ZCTA table (geo-friendly numeric cols)
cs.execute("""
CREATE OR REPLACE TABLE ZCTA_2021 (
  GEOID        STRING,
  ALAND        NUMBER,
  AWATER       NUMBER,
  ALAND_SQMI   FLOAT,
  AWATER_SQMI  FLOAT,
  INTPTLAT     FLOAT,
  INTPTLONG    FLOAT
)
""")
cs.execute("""
INSERT INTO ZCTA_2021 (
  GEOID, ALAND, AWATER, ALAND_SQMI, AWATER_SQMI, INTPTLAT, INTPTLONG
)
SELECT
  GEOID,
  TRY_TO_NUMBER(ALAND),
  TRY_TO_NUMBER(AWATER),
  TRY_CAST(ALAND_SQMI  AS FLOAT),
  TRY_CAST(AWATER_SQMI AS FLOAT),
  TRY_CAST(INTPTLAT    AS FLOAT),
  TRY_CAST(INTPTLONG   AS FLOAT)
FROM ZCTA_2021_RAW
""")
cs.execute("SELECT COUNT(*) FROM ZCTA_2021")
print("rows:", cs.fetchone()[0])
cs.execute("SELECT * FROM ZCTA_2021 LIMIT 5")
print(cs.fetchall())

"""After loading the TXT file into a table, we ended up with 33,791 distinct rows."""

# (3.2) Supplier invoice XML → VARIANT RAW → typed table
# For this task, we created a separate stage for the XML file.
cs.execute("CREATE OR REPLACE STAGE xmldata")
cs.execute("""
           CREATE OR REPLACE FILE FORMAT my_xml_ff
           TYPE = XML
           STRIP_OUTER_ELEMENT = TRUE""")
cs.execute("""
           PUT file:///home/jovyan/msba/MGTA464/Data/SupplierTransactionsXML.xml @xmldata AUTO_COMPRESS=TRUE;
           """)

# Shred XML into a typed relational table (each row = one invoice)
cs.execute("CREATE OR REPLACE TABLE supplier_trans_raw (src VARIANT)")
cs.execute("""COPY INTO supplier_trans_raw
FROM @xmldata
FILE_FORMAT = (FORMAT_NAME = 'my_xml_ff');""")

# Note this query was debugged with the help of ChatGPT.
cs.execute("""
           CREATE OR REPLACE TABLE supplier_trans AS
SELECT
  TRY_TO_NUMBER(XMLGET(src,'SupplierTransactionID'):"$"::string)                      AS SupplierTransactionID,
  TRY_TO_NUMBER(XMLGET(src,'SupplierID'):"$"::string)                                  AS SupplierID,
  TRY_TO_NUMBER(XMLGET(src,'TransactionTypeID'):"$"::string)                           AS TransactionTypeID,
  TRY_TO_NUMBER(NULLIF(XMLGET(src,'PurchaseOrderID'):"$"::string, ''))                 AS PurchaseOrderID,
  TRY_TO_NUMBER(XMLGET(src,'PaymentMethodID'):"$"::string)                              AS PaymentMethodID,
  NULLIF(XMLGET(src,'SupplierInvoiceNumber'):"$"::string, '')                           AS SupplierInvoiceNumber,
  TRY_TO_DATE(SPLIT_PART(XMLGET(src,'TransactionDate'):"$"::string,' ',1),'YYYY-MM-DD') AS TransactionDate,
  TRY_TO_DECIMAL(XMLGET(src,'AmountExcludingTax'):"$"::string,38,2)                    AS AmountExcludingTax,
  TRY_TO_DECIMAL(XMLGET(src,'TaxAmount'):"$"::string,38,2)                              AS TaxAmount,
  TRY_TO_DECIMAL(XMLGET(src,'TransactionAmount'):"$"::string,38,2)                      AS TransactionAmount,
  TRY_TO_DECIMAL(XMLGET(src,'OutstandingBalance'):"$"::string,38,2)                     AS OutstandingBalance,
  TRY_TO_DATE(SPLIT_PART(XMLGET(src,'FinalizationDate'):"$"::string,' ',1),'YYYY-MM-DD') AS FinalizationDate,
  IFF(TRY_TO_NUMBER(XMLGET(src,'IsFinalized'):"$"::string)=1, TRUE, FALSE)              AS IsFinalized,
  TRY_TO_NUMBER(XMLGET(src,'LastEditedBy'):"$"::string)                                  AS LastEditedBy,
  TRY_TO_TIMESTAMP_NTZ(XMLGET(src,'LastEditedWhen'):"$"::string)                         AS LastEditedWhen
FROM supplier_trans_raw;

""")

"""After loading the XML file into the table, we ended up with 2,438 rows."""

# Quick check on PO amount from my_table_raw per assignment guidelines.
cs.execute("""SELECT receivedouters * expectedunitpriceperouter AS POAmount
           FROM my_table_raw;""")
print(cs.fetchall())


# Task 6a — Extract supplier_case from Postgres, stage, COPY into Snowflake

# Export supplier_case from Postgres to local CSV WITHOUT loading into Python memory
# Note the functions and methods from these libraries were used with the help of ChatGPT.
import psycopg2, os

# For the Postgres connection, we assigned the hostname, database name, user, password, and port.
PG_HOST = "127.0.0.1"
PG_DB = "WestCoastImporters"
PG_USER = "jovyan"
PG_PASSWORD = "postgres"
PG_PORT = 8765

csv_path = "/home/jovyan/msba/mgta464/Data/supplier_case.csv"
os.makedirs(os.path.dirname(csv_path), exist_ok=True)

pg_conn = psycopg2.connect(
    host=PG_HOST, dbname=PG_DB, user=PG_USER, password=PG_PASSWORD, port=PG_PORT
)
with (
    pg_conn,
    pg_conn.cursor() as pgcur,
    open(csv_path, "w", newline="", encoding="utf-8") as f,
):
    pgcur.copy_expert(
        """
    COPY (SELECT * FROM public.supplier_case)
    TO STDOUT WITH (FORMAT CSV, HEADER, DELIMITER ',', NULL '\\N')
""",
        f,
    )
print("Wrote:", csv_path)

# Now that we have extracted the data from the supplier_case Postgres file, we can now prep the raw table and stage the data to be loaded into Snowflake via COPY.
cs.execute("USE DATABASE TESTDB")
cs.execute("USE SCHEMA PUBLIC")

cs.execute("""
CREATE OR REPLACE TABLE SUPPLIER_CASE_RAW (
  supplierid                 STRING,
  suppliername               STRING,
  suppliercategoryid         STRING,
  primarycontactpersonid     STRING,
  alternatecontactpersonid   STRING,
  deliverymethodid           STRING,
  postalcityid               STRING,
  supplierreference          STRING,
  bankaccountname            STRING,
  bankaccountbranch          STRING,
  bankaccountcode            STRING,
  bankaccountnumber          STRING,
  bankinternationalcode      STRING,
  paymentdays                STRING,
  internalcomments           STRING,
  phonenumber                STRING,
  faxnumber                  STRING,
  websiteurl                 STRING,
  deliveryaddressline1       STRING,
  deliveryaddressline2       STRING,
  deliverypostalcode         STRING,
  deliverylocation           STRING,
  postaladdressline1         STRING,
  postaladdressline2         STRING,
  postalpostalcode           STRING,
  lasteditedby               STRING,
  validfrom                  STRING,
  validto                    STRING
)
""")

csv_path = "file:///home/jovyan/msba/mgta464/Data/supplier_case.csv"

cs.execute("""
CREATE OR REPLACE FILE FORMAT PG_CSV
  TYPE=CSV
  PARSE_HEADER=TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY='\"'
  TRIM_SPACE=TRUE
  NULL_IF=('','\\N','NULL')
""")

cs.execute(
    f"PUT {csv_path} @%SUPPLIER_CASE_RAW/inbox/ OVERWRITE=TRUE AUTO_COMPRESS=TRUE"
)

cs.execute("""
COPY INTO SUPPLIER_CASE_RAW
FROM @%SUPPLIER_CASE_RAW/inbox/
FILE_FORMAT=(FORMAT_NAME=PG_CSV)
MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE
PATTERN='.*\\.csv(\\.gz)?'
ON_ERROR=CONTINUE
FORCE=TRUE
""")

"""The supplier_case_raw table from the Postgres file shows 13 distinct suppliers."""

# Task 4–5 — Build PO header, choose one invoice per PO, join, and (optionally) create materialized view purchase_orders_and_invoices

# For this task, we started by creating a view called PO_HEADER with one row per (PurchaseOrderID, SupplierID) with key dates, finalized flag, and PO amount.
cs.execute("""
CREATE OR REPLACE VIEW PO_HEADER AS
SELECT
  PURCHASEORDERID,
  SUPPLIERID,
  MIN(ORDERDATE)                  AS ORDERDATE,
  MIN(EXPECTEDDELIVERYDATE)       AS EXPECTEDDELIVERYDATE,
  MAX(LASTRECEIPTDATE)            AS LASTRECEIPTDATE,
  BOOLAND_AGG(ISORDERFINALIZED)   AS ISORDERFINALIZED
  ROUND(SUM(COALESCE(ORDEREDOUTERS,0) * COALESCE(EXPECTEDUNITPRICEPEROUTER,0)), 2) AS POAMOUNT
FROM MY_TABLE
GROUP BY PURCHASEORDERID, SUPPLIERID
""")
cs.execute("SELECT COUNT(*) FROM PO_HEADER")
print("PO_HEADER rows:", cs.fetchone()[0])

# Choose one invoice per PO
# Note this query was developed with the help of ChatGPT.
cs.execute("""
CREATE OR REPLACE VIEW INVOICE_ONEPER_PO AS
SELECT *
FROM (
  SELECT
    SUPPLIERTRANSACTIONID,
    SUPPLIERID,
    TRANSACTIONTYPEID,
    PURCHASEORDERID,
    PAYMENTMETHODID,
    SUPPLIERINVOICENUMBER,
    TRANSACTIONDATE,
    AMOUNTEXCLUDINGTAX,
    TAXAMOUNT,
    TRANSACTIONAMOUNT,
    OUTSTANDINGBALANCE,
    FINALIZATIONDATE,
    ISFINALIZED,
    LASTEDITEDBY,
    LASTEDITEDWHEN,
    ROW_NUMBER() OVER (
      PARTITION BY PURCHASEORDERID
      ORDER BY LASTEDITEDWHEN DESC NULLS LAST, TRANSACTIONDATE DESC, SUPPLIERTRANSACTIONID
    ) AS RN
  FROM SUPPLIER_TRANS
)
WHERE RN = 1
""")


# Diagnostics: how many POs have >1 invoice in raw (sanity check)
cs.execute("""
SELECT COUNT(*) FROM (
  SELECT PURCHASEORDERID FROM SUPPLIER_TRANS
  GROUP BY 1 HAVING COUNT(*) > 1
)
""")
print("POs with >1 invoice:", cs.fetchone()[0])

# Join header + chosen invoice
cs.execute("""
CREATE OR REPLACE TABLE PO_INVOICE AS
SELECT
  h.PURCHASEORDERID,
  h.SUPPLIERID,
  h.ORDERDATE,
  h.EXPECTEDDELIVERYDATE,
  h.LASTRECEIPTDATE,
  h.ISORDERFINALIZED,

  i.SUPPLIERTRANSACTIONID,
  i.SUPPLIERINVOICENUMBER,
  i.TRANSACTIONDATE,
  i.AMOUNTEXCLUDINGTAX,
  i.TAXAMOUNT,
  i.TRANSACTIONAMOUNT,
  i.OUTSTANDINGBALANCE,
  i.FINALIZATIONDATE,
  i.ISFINALIZED     AS INVOICE_FINALIZED,
  i.LASTEDITEDWHEN  AS INVOICE_LASTEDITEDWHEN
FROM PO_HEADER h
JOIN INVOICE_ONEPER_PO i
  ON i.PURCHASEORDERID = h.PURCHASEORDERID
 AND i.SUPPLIERID      = h.SUPPLIERID
""")

# Check row count
cs.execute("SELECT COUNT(*) FROM PO_INVOICE")
print("PO_INVOICE rows:", cs.fetchone()[0])

# Using the joined data, we then created a materialized view called purchase_orders_and_invoices to show the difference between AmountExcludingTax and POAmount.
cs.execute("""
CREATE OR REPLACE MATERIALIZED VIEW PURCHASE_ORDERS_AND_INVOICES AS
SELECT
  *,
  COALESCE(AMOUNTEXCLUDINGTAX,0) - COALESCE(POAMOUNT,0) AS INVOICED_VS_QUOTED
FROM PO_INVOICE;
""")

# Quick sample checks
cs.execute("""SELECT COUNT(*) AS ROW_COUNT FROM PURCHASE_ORDERS_AND_INVOICES;
SELECT PURCHASEORDERID, POAMOUNT, AMOUNTEXCLUDINGTAX, INVOICED_VS_QUOTED
FROM PURCHASE_ORDERS_AND_INVOICES
ORDER BY PURCHASEORDERID
LIMIT 10;""")
print(cs.fetchall())
cs.execute("SELECT * FROM PO_INVOICE ORDER BY ORDERDATE LIMIT 10")
print(cs.fetchall())


# Task 6b — Infer CREATE TABLE from CSV

# Note the code used in this task was generated by ChatGPT.
import csv, re, os
from datetime import datetime

# Helpers and patterns
NULLS = {"", "null", "NULL", "\\N"}
BOOLS = {"true", "false", "t", "f", "yes", "no", "y", "n", "1", "0"}
DATE_FMTS = ["%Y-%m-%d", "%m/%d/%Y", "%Y/%m/%d"]
TIME_FMTS = ["%H:%M", "%H:%M:%S", "%H:%M:%S.%f", "%H:%M.%f"]
TS_FMTS = [
    "%Y-%m-%d %H:%M",
    "%Y-%m-%d %H:%M:%S",
    "%Y-%m-%d %H:%M:%S.%f",
    "%Y/%m/%d %H:%M",
    "%Y/%m/%d %H:%M:%S",
    "%Y/%m/%d %H:%M:%S.%f",
]
int_re = re.compile(r"^[+-]?\d+$")
dec_re = re.compile(r"^[+-]?\d+\.\d+$")
sci_re = re.compile(r"^[+-]?\d+(\.\d+)?[eE][+-]?\d+$")


def _norm(name: str) -> str:
    """Normalize a header into a safe Snowflake identifier (UPPER, underscores)."""
    n = re.sub(r"[^A-Za-z0-9_]", "_", name.strip())
    if n and n[0].isdigit():
        n = "_" + n
    return (re.sub(r"_+", "_", n).strip("_") or "COL").upper()


def _try_any(v, fmts):
    """Return True if value parses under any of the provided datetime formats."""
    for f in fmts:
        try:
            datetime.strptime(v, f)
            return True
        except ValueError:
            pass
    return False


def _num_p_s(val: str):
    """Compute precision/scale for a numeric literal (bounded for Snowflake)."""
    s = val.lstrip("+-")
    if "." in s:
        ip, fp = s.split(".", 1)
        return min(len(ip) + len(fp), 38), min(len(fp), 12)
    return min(len(s), 38), 0


# The following function was **generated by ChatGPT** (per Case Task 6b).
# It infers Snowflake column types from a CSV header + sampled data and returns a CREATE TABLE statement you can execute via cs.execute(...).
def infer_snowflake_ddl(
    csv_path: str,
    table_name: str,
    overrides: dict | None = None,
    sample_rows: int = 5000,
) -> tuple[str, list[dict]]:
    """
    (Generated by ChatGPT) Infer Snowflake CREATE TABLE DDL from a CSV.

    Args:
      csv_path: Path to CSV file with header row.
      table_name: Target Snowflake table name (will be normalized/UPPERcased).
      overrides: Optional dict of {lowercased_column_name: 'SNOWFLAKE_TYPE'}
                 to force a specific type for a column.
      sample_rows: Max sample rows for inference (0 → full scan).

    Returns:
      (ddl_sql, columns_meta)
        ddl_sql: 'CREATE OR REPLACE TABLE <TABLE> (...)' string
        columns_meta: list of dicts {name: <normalized>, type: <TYPE>, source: <original_header>}

    Types inferred conservatively among:
      NUMBER(p[,s]), FLOAT, BOOLEAN, DATE, TIME, TIMESTAMP_NTZ, STRING
    """
    overrides = {k.lower(): v for k, v in (overrides or {}).items()}

    # Read header + a sample of rows (streaming)
    with open(csv_path, newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        cols = rdr.fieldnames
        buckets = {c: [] for c in cols}
        for i, row in enumerate(rdr):
            if i >= sample_rows:
                break
            for c in cols:
                v = (row[c] or "").strip()
                if v in NULLS:
                    continue
                buckets[c].append(v)

    def guess(col: str, values: list[str]) -> str:
        # Explicit override
        ovr = overrides.get(col.lower())
        if ovr:
            return ovr

        if col.lower().endswith("id") or col.lower() in {"paymentdays", "lasteditedby"}:
            return "NUMBER"

        if not values:
            return "STRING"

        low = [v.lower() for v in values]

        if all(v in BOOLS for v in low):
            return "BOOLEAN"
        if all(_try_any(v, DATE_FMTS) for v in values):
            return "DATE"
        if all(_try_any(v.replace("T", " ").replace("Z", ""), TS_FMTS) for v in values):
            return "TIMESTAMP_NTZ"
        if all(_try_any(v, TIME_FMTS) for v in values):
            return "TIME"

        if all(int_re.fullmatch(v) for v in values):
            p = max(len(v.lstrip("+-")) for v in values)
            return f"NUMBER({min(max(p, 1), 38)})"

        if all(dec_re.fullmatch(v) or int_re.fullmatch(v) for v in values):
            maxp = maxs = 0
            for v in values:
                p, s = _num_p_s(v)
                maxp, maxs = max(maxp, p), max(maxs, s)
            return f"NUMBER({maxp},{maxs})" if maxs > 0 else f"NUMBER({maxp})"

        if all(sci_re.fullmatch(v) for v in values):
            return "FLOAT"

        return "STRING"

    names_norm = [_norm(c) for c in cols]
    types = [guess(c, buckets[c]) for c in cols]

    tname = _norm(table_name or os.path.splitext(os.path.basename(csv_path))[0])
    ddl = (
        "CREATE OR REPLACE TABLE "
        + tname
        + " (\n  "
        + ",\n  ".join(f"{n} {t}" for n, t in zip(names_norm, types))
        + "\n);"
    )

    meta = [
        {"name": n, "type": t, "source": c} for n, t, c in zip(names_norm, types, cols)
    ]
    return ddl, meta


# Example: infer DDL for supplier_case and execute it
ddl, cols = infer_snowflake_ddl(
    "/home/jovyan/msba/mgta464/Data/supplier_case.csv",
    table_name="SUPPLIER_CASE",
    overrides={
        # example overrides for known string-like fields
        "validfrom": "STRING",
        "validto": "STRING",
        "deliverylocation": "STRING",
    },
)
print(ddl)  # inspect
cs.execute(ddl)  # create table from inferred schema (then COPY as desired)
print(cs.fetchall())


# Task 7 (a & b) - ZIP → Lat/Lon → Nearest Weather Station (TMAX) → Daily Temps

# Note the queries for this part were put together with help from ChatGPT, as well as the approach used in
# https://medium.com/data-science/noaa-weather-data-in-snowflake-free-20e90ee916ed.

"""
This module builds the ZIP geocoding and nearest-station mapping (7a) and then
materializes daily maximum temperature (TMAX) by ZIP (7b), using the Cybersyn
Weather & Environment Marketplace data in Snowflake.

Pipeline
  1) Context: USE DATABASE/SCHEMA
  2) 7a. Extract ZIP codes from supplier data (postalpostalcode)
  3) 7a. Clean ZCTA (ZIP → centroid lat/lon) view
  4) 7a. Join the supplier ZIPs to ZCTA centroids (geocoded)
  5) 7a. Stations that actually have 'maximum_temperature' observations
  6) 7a. Nearest TMAX-reporting station per ZIP (Haversine distance)
  7) 7b. Pull TMAX time series for chosen stations → table
  8) 7b. ZIP × DATE × HIGH_TEMPERATURE table and a materialized view

Notes:
  - We normalize ZIPs to 5 chars (zero-padded), preserving leading zeros.
  - Haversine distance is computed in kilometers (Earth radius ~ 6371 km).
"""

# Use database and schema
cs.execute("USE DATABASE TESTDB")
cs.execute("USE SCHEMA PUBLIC")

# 7a — Supplier ZIPs (normalized 5-digit)
# Extract 5-digit ZIPs from SUPPLIER_CASE_RAW.POSTALPOSTALCODE
# Any embedded ZIP (1–5 digits) is captured, then LPAD to 5.
cs.execute(r"""
CREATE OR REPLACE VIEW SUPPLIER_ZIPS AS
SELECT DISTINCT
  LPAD(REGEXP_SUBSTR(POSTALPOSTALCODE, '\d{1,5}'), 5, '0') AS ZIP
FROM SUPPLIER_CASE_RAW
WHERE REGEXP_SUBSTR(POSTALPOSTALCODE, '\d{1,5}') IS NOT NULL
""")

# How many distinct supplier ZIPs (sanity check)
cs.execute("SELECT COUNT(*) FROM SUPPLIER_ZIPS")
print("supplier_zip_count:", cs.fetchone()[0])

"""The supplier_zips view shows 8 distinct rows."""

# 7a — Clean ZCTA view (normalize keys & types)
cs.execute("""
CREATE OR REPLACE VIEW ZCTA_2021_CLEAN AS
SELECT
  LPAD(TO_VARCHAR(GEOID), 5, '0') AS ZIP,
  TRY_TO_DOUBLE(INTPTLAT)          AS LAT,
  TRY_TO_DOUBLE(INTPTLONG)         AS LON
FROM ZCTA_2021
""")
cs.execute("SELECT COUNT(*) FROM ZCTA_2021_CLEAN")
print("zcta_rows:", cs.fetchone()[0])

# 7a — Supplier ZIPs with centroid coordinates (ZIP → LAT/LON)
cs.execute("""
CREATE OR REPLACE VIEW SUPPLIER_ZIPS_GEOCODED AS
SELECT s.ZIP, z.LAT, z.LON
FROM SUPPLIER_ZIPS s
JOIN ZCTA_2021_CLEAN z USING (ZIP)
""")
cs.execute("SELECT COUNT(*) FROM SUPPLIER_ZIPS_GEOCODED")
print("geocoded_zip_count:", cs.fetchone()[0])

# 7a - We then created a temp table with stations that actually report 'maximum_temperature' based on
# the weather and environment data from Snowflake marketplace.
cs.execute("""
CREATE OR REPLACE TEMP TABLE STATIONS_TMAX AS
SELECT
  si.NOAA_WEATHER_STATION_ID,
  si.NOAA_WEATHER_STATION_NAME,
  si.LATITUDE,
  si.LONGITUDE
FROM WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX si
JOIN (
  SELECT DISTINCT NOAA_WEATHER_STATION_ID
  FROM WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES
  WHERE VARIABLE = 'maximum_temperature'
) t ON t.NOAA_WEATHER_STATION_ID = si.NOAA_WEATHER_STATION_ID
""")
cs.execute("SELECT COUNT(*) FROM STATIONS_TMAX")
print("stations_with_tmax:", cs.fetchone()[0])

"""The stations_tmax temp table shows 22,537 stations that report 'maximum temperature'."""

# 7a — We then created a view showing the nearest TMAX-reporting station per ZIP (Haversine in KM)
# CROSS JOIN Z (geocoded ZIPs) × S (stations) + QUALIFY ROW_NUMBER() = 1
# Note the cross join, distance calculations, and trig functions of this query were utilized with the help of ChatGPT.
cs.execute("""
CREATE OR REPLACE VIEW ZIP_NEAREST_STATION_TMAX AS
WITH Z AS (
  SELECT ZIP, CAST(LAT AS FLOAT) AS LAT, CAST(LON AS FLOAT) AS LON
  FROM SUPPLIER_ZIPS_GEOCODED
),
S AS (
  SELECT NOAA_WEATHER_STATION_ID, NOAA_WEATHER_STATION_NAME, LATITUDE, LONGITUDE
  FROM STATIONS_TMAX
)
SELECT
  z.ZIP,
  s.NOAA_WEATHER_STATION_ID,
  s.NOAA_WEATHER_STATION_NAME,
  /* Haversine distance (Earth radius ≈ 6371 km) */
  2 * 6371 * ASIN(
    SQRT(
      POWER(SIN(RADIANS((s.LATITUDE  - z.LAT)/2)), 2) +
      COS(RADIANS(z.LAT)) * COS(RADIANS(s.LATITUDE)) *
      POWER(SIN(RADIANS((s.LONGITUDE - z.LON)/2)), 2)
    )
  ) AS DIST_KM
FROM Z z
CROSS JOIN S s
QUALIFY ROW_NUMBER() OVER (PARTITION BY z.ZIP ORDER BY DIST_KM) = 1
""")

# Quick check
cs.execute("SELECT * FROM ZIP_NEAREST_STATION_TMAX ORDER BY ZIP LIMIT 10")
print("sample ZIP→station:", cs.fetchall())

# 7b — Pull TMAX time series for the chosen stations (local table)
# This limits the large marketplace table to only what we need.
cs.execute("""
CREATE OR REPLACE TABLE NOAA_TMAX_LOCAL AS
SELECT
  NOAA_WEATHER_STATION_ID,
  DATE::DATE AS DATE,
  VALUE      AS HIGH_TEMPERATURE
FROM WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES
WHERE VARIABLE = 'maximum_temperature'
  AND NOAA_WEATHER_STATION_ID IN (
      SELECT NOAA_WEATHER_STATION_ID FROM ZIP_NEAREST_STATION_TMAX
  )
""")
cs.execute("SELECT COUNT(*) FROM NOAA_TMAX_LOCAL")
print("tmax_rows (filtered to chosen stations):", cs.fetchone()[0])

# 7b — We then built a ZIP×DATE high-temperature table, then a MV for easy use.
cs.execute("""
CREATE OR REPLACE TABLE ZIP_TMAX AS
SELECT ZIP_CODE, DATE, HIGH_TEMPERATURE
FROM (
  SELECT
    z.ZIP AS ZIP_CODE,
    t.DATE,
    t.HIGH_TEMPERATURE,
    ROW_NUMBER() OVER (PARTITION BY z.ZIP, t.DATE ORDER BY t.DATE) AS rn
  FROM ZIP_NEAREST_STATION_TMAX z
  JOIN NOAA_TMAX_LOCAL t
    ON t.NOAA_WEATHER_STATION_ID = z.NOAA_WEATHER_STATION_ID
)
QUALIFY rn = 1
""")

# Materialized view for downstream queries/joins
cs.execute("""
CREATE OR REPLACE MATERIALIZED VIEW SUPPLIER_ZIP_CODE_WEATHER AS
SELECT ZIP_CODE, DATE, HIGH_TEMPERATURE
FROM ZIP_TMAX
""")

# Row count check
cs.execute("SELECT COUNT(*) FROM SUPPLIER_ZIP_CODE_WEATHER")
print("mv row_count:", cs.fetchone()[0])

# Quick peek at MV
cs.execute("SELECT * FROM SUPPLIER_ZIP_CODE_WEATHER ORDER BY ZIP_CODE, DATE LIMIT 20")
print("sample weather rows:", cs.fetchall())

"""After joining the supplier ZIPs with the weather stations, we ended up with a MV with 49,132 rows."""


# Task 8 — Final Join: Transactions × Supplier ZIP × Weather (matched only)

"""
Inputs
  - PURCHASE_ORDERS_AND_INVOICES   : transaction-level facts (has TRANSACTIONDATE)
  - SUPPLIER_CASE_RAW              : raw supplier data with POSTALPOSTALCODE
  - SUPPLIER_ZIP_CODE_WEATHER      : ZIP_CODE, DATE, HIGH_TEMPERATURE from Step 7b

Output
  - View TXN_WITH_TEMPERATURE:
      One row per transaction where a weather reading exists for the supplier’s ZIP
      on the transaction date.
"""

# We first created a view that contains supplierid and 5 digit ZIPs from supplier_case_raw
# This view will be used to join onto purchase_orders_and_invoices and supplier_zip_code_weather.
cs.execute("""CREATE OR REPLACE VIEW SUPPLIER_CASE_ZIP AS
SELECT
    DISTINCT TO_NUMBER(supplierid) AS SUPPLIERID,
    LPAD(REGEXP_SUBSTR(POSTALPOSTALCODE, '\\d{1,5}'), 5, '0') AS ZIP
FROM SUPPLIER_CASE_RAW
WHERE REGEXP_SUBSTR(POSTALPOSTALCODE, '\\d{1,5}') IS NOT NULL
""")

# We then created the final join.
cs.exexute("""CREATE OR REPLACE VIEW TXN_WITH_TEMPERATURE AS
(SELECT
  -- keys
  poi.PURCHASEORDERID,
  poi.SUPPLIERID,
  CAST(poi.TRANSACTIONDATE AS DATE)       AS TXN_DATE,
  scz.ZIP                              AS ZIP_CODE,

  wz.HIGH_TEMPERATURE,

  poi.ORDERDATE,
  poi.EXPECTEDDELIVERYDATE,
  poi.LASTRECEIPTDATE,
  poi.ISORDERFINALIZED,
  poi.SUPPLIERINVOICENUMBER,
  poi.AMOUNTEXCLUDINGTAX,
  poi.TAXAMOUNT,
  poi.TRANSACTIONAMOUNT,
  poi.OUTSTANDINGBALANCE,
  poi.FINALIZATIONDATE,
  poi.INVOICE_FINALIZED,
  poi.INVOICE_LASTEDITEDWHEN

FROM PURCHASE_ORDERS_AND_INVOICES poi
JOIN SUPPLIER_CASE_ZIP scz
  ON scz.SUPPLIERID = poi.SUPPLIERID
JOIN SUPPLIER_ZIP_CODE_WEATHER wz
  ON wz.ZIP_CODE = scz.ZIP
 AND wz.DATE = CAST(poi.TRANSACTIONDATE AS DATE))
""")

# Quick sanity check
cs.execute("SELECT COUNT(*) FROM TXN_WITH_TEMPERATURE")
print("rows with matched temps:", cs.fetchone()[0])

# Quick peek at final view
cs.execute("""
SELECT ZIP_CODE, TXN_DATE, HIGH_TEMPERATURE, PURCHASEORDERID, SUPPLIERINVOICENUMBER
FROM TXN_WITH_TEMPERATURE
ORDER BY TXN_DATE, ZIP_CODE
LIMIT 10
""")
print(cs.fetchall())

"""After joining purchase_orders_and_invoices, supplier_case, and supplier_zip_code_weather, we ended up with a view showing 1,497 rows with only transactions that have matching temperature readings."""
